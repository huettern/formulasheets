

% Communication Systems D-ITET
% ===========================================================================
% @Author: Noah Huetter
% @Date:   2019-09-24 17:26:28
% @Last Modified by:   noah
% @Last Modified time: 2019-11-14 13:56:16
% ---------------------------------------------------------------------------

\documentclass[a4paper, fontsize=8pt, landscape, DIV=1]{scrartcl}
\usepackage{lastpage}
\usepackage{hyperref}
% Include general settings and customized commands
\input{settings/general}
\input{settings/commands}

% This package makes formulas a bit more compact but less beautiful
% \usepackage{newtxtext,newtxmath}


% \bibliography{semiconductordevices}
% \bibliographystyle{ieeetr}
\medmuskip=1mu

%change page style for header
\pagestyle{fancy}
\footskip 20pt

% Uncomment this line to make formulasheet ultra compact
% This removes
% - list of variables
% \newcommand{\makeultracompact}{irrelevant}
\let\makeultracompact\undefined

% Make stuff ultra compact if so desired
\ifdefined\makeultracompact
  \setlength{\parskip}{0pt}
  \setlength{\abovedisplayskip}{0pt}
  \setlength{\belowdisplayskip}{0pt}
  \setlength{\abovedisplayshortskip}{0pt}
  \setlength{\belowdisplayshortskip}{0pt}
\else
\fi
 

\newcommand{\EIRP}{\text{EIRP}}
\newcommand{\SNR}{\text{SNR}}

% -----------------------------------------------------------------------
\IfFileExists{../build/revision.tex}{
  \input{../build/revision.tex}
  \rhead{Compiled: \compiledate \hspace{1em} on: \hostname \hspace{1em} git-sha: \revision \hspace{1em} Noah Huetter}
}{\rhead{Noah Huetter}}

\ifdefined\makeultracompact
  \lhead{ETH Communication Systems 2019 \hspace{1em}compact version}
\else
  \lhead{ETH Communication Systems 2019}
\fi
\chead{\thepage}
\cfoot{}
\headheight 17pt \headsep 10pt
\title{ETH Communication Systems 2019}
\author{Noah Huetter}

\date{\today}
\begin{document}

\setcounter{page}{0}
\setcounter{secnumdepth}{2} %no enumeration of sections
\begin{multicols*}{4}
	\section*{Disclaimer}
	This summary is part of the lecture ``ETH Communication Systems'' (227-0121-00) by Prof. Dr. Armin Wittneben (FS19). It is based on the lecture. \\[6pt]
	Please report errors to \href{mailto:huettern@student.ethz.ch}{huettern@student.ethz.ch} such that others can benefit as well.\\[6pt]	
  The upstream repository can be found at \href{https://github.com/noah95/formulasheets}{https://github.com/noah95/formulasheets}
	\vfill\null
  \columnbreak
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \tableofcontents
  \vfill\null
  %\columnbreak
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\pagebreak
  \maketitle 
  \setcounter{page}{1}
  \thispagestyle{fancy}

  % ---------------------------------------------------------------------------
  \section{Random Processes}
  % ---------------------------------------------------------------------------
  % What is
  \cgraphic{0.8}{img/rp.png}
  A random process $X(t)$:
  \begin{itemize}
    \item is a sample space composed of (real valued) time functions: 
      $\{x_1(t), x_2(t), \dots, x_n(t)\}$
    \item observed at a fixed $t_k$ is a random variable 
      $X(t_k) = \{x_1(t_k), x_2(t_k), \dots, x_n(t_k)\}$
    \item The time function $x_s(t)$ is a \textbf{realization} (sample function)
    \item $x_s(t_k)$ observed at $t_k$ is a real number
    \item A stochastic process consists of infinitely many random variables, 
      one for each $t_k$, with the CDF $F_{\{X(t_k)\}}(x) = P(X(t_k)\leq x)$
  \end{itemize}


  \subsection{Stationary processes}
  A process is \textbf{Strict Sense Stationary (SSS)} if:
  \begin{itemize}
    \item $X(t)$ and $X(t+\tau)$ have same satistics $\forall \tau$
    \item The joint distribution function of a set of r.v. observed at times
      $t_1,\dots,t_n$ is invariant to a time-shift.
  \end{itemize}
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      \forall n,\tau,t_1,\dots,t_n: \\
      F_{\{X(t_1+\tau),X(t_2+\tau),\dots,X(t_n+\tau)\}}(x_1,x_2,\dots,x_n) = \\
        F_{\{X(t_1),X(t_2),\dots,X(t_n)\}}(x_1,x_2,\dots,x_n)
    \end{gathered}
  \end{empheq}

  Properties:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      \forall t_k : \mu_X(t_k) = \mu_X \\
      \forall t_1, t_2 : R_X(t_1, t_2) = R_X(t_2-t_1) = R_X(\tau) \\
      C_X(t_1, t_2) = \E[(X(t_1)-\mu_X)(X(t_2)-\mu_X)] \\ = R_X(t_2-t_1) - \mu_X^2 \\
    \end{gathered}
  \end{empheq}
  
  A process is \textbf{Wide Sense Stationary (WSS)} if a r.p. has a \textit{constant} mean and the autocorrelation depends only on the \textit{time difference}.
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      \forall t : \mu_X(t) = \mu_X \\
      \forall t_1, t_2 : R_X(t_1, t_2) = R_X(t_2-t_1) = R_X(\tau)
    \end{gathered}
  \end{empheq}

  Strict sense stationary $\implies$ wide sense stationary.


  \subsection{Mean and correlation}
  Defined as expectation of r.v. $X(t_k)$ by observing process at time $t_k$.
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      \mu_X(t_k) = \E[X(t_k)] = \intinf x f_{\{X(t_k)\}}(x) \dx \\
    \end{gathered}
  \end{empheq}

  Autocorrelation function $R_X$ and autovariance function $C_X$ of a random process:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      R_{X}(t_{1},t_{2}) = \E[X(t_{1})X(t_{2})] \triangleq \\
      \intinf\intinf x_{1}x_{2}f_{X_{1},X_{2}}(x_{1}, x_{2})\dx_{1}\dx_{2}\\
      R_{XY}(x,y) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xyf_{X,Y}(x, y)dxdy\\
      C_{X}(t_{1},t_{2}) = R_{X}(t_{2}-t_{1}) - m_{X}^{2}
    \end{gathered}
  \end{empheq}

  \begin{itemize}
    \item The mean and autocorrelation function determine the autocovariance function
    \item The mean and autocorrelation function only describe the first two moments of the process
  \end{itemize}

  Properties of the autocorrelation function:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{align*}
      \E[X^2(t)] &= R_X(0) & R_X(\tau) &= R_X(-\tau) \\
      \abs{R_X(\tau)} &\leq R_X(0)
    \end{align*}
  \end{empheq}
  \cgraphic{0.8}{img/autocorrelation.png}
  
  The Cross-correlation function $R_{XY}(t,u)$ of two random processes:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      R_{XY}(t,u) = \E[X(t)Y(u)] = \\
      \intinf xy\cdot f_{X,Y}(x, y) \dx \dy\\
    \end{gathered}
  \end{empheq}
  \begin{itemize}
    \item Stationariy menas $R_{XY}(t,u) = R_{XY}(\tau)$ for $\tau=t-u$
    \item Not generally an even function of $t$
    \item Not necessarily a maximum at $\tau=0$
    \item Symmetry: $R_{XY}(\tau) = R_{XY}(-\tau)$
  \end{itemize}

  \subsection{Ergodicity}
  Definition: A random process is \textit{ergodic} in the mean if
  \begin{itemize}
    \item Time average approaches ensemble averages for increasing $T$
    \item The variance of the time average approaches zero for incr. $T$
  \end{itemize}
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{align*}
      \lim_{T\to\infty} \mu_X(T) &= \mu_X & \lim_{T\to\infty} \Var[\mu_X(T)] &= 0
    \end{align*}
  \end{empheq}

  Or in other words: The same behavior averaged over time as averaged over the space of all the system's states.

  \subsection{Filtered processes}
  Stationary random process $X(t)$ is input to a linear timeinvariant (LTI) filter with impulse response $h(t)$.
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      Y(t) = \intinf h(\tau_1)X(t-\tau_1)\dtau_1 \\ S_Y(f) = \abs{H(f)}^2S_X(f)
    \end{gathered}
  \end{empheq}

  Find mean and autocorrelation of $Y(t)$:
  \begin{empheq}{gather*}
      \mu_X = \E[X(t)] \quad R_X(\tau) = \E[X(t)X(t-\tau)] \\
      \mu_Y = \E[Y(t)] = \E\left[\intinf h(\tau_1)X(t-\tau_1)\dtau_1\right]
  \end{empheq}

  Can interchange expectation and integration if stable $\intinf\abs{h(t)}\dt<\infty$ and finite mean $\mu_X<\infty$
  \begin{empheq}[box=\eqbox]{gather*}
      \mu_Y = \intinf h(\tau_1)\E[X(t-\tau_1)]\dtau_1 = \mu_X \intinf h(\tau_1) \dtau_1
  \end{empheq}

  Autocorrelation:
  \begin{empheq}{gather*}
      R_Y(t,u) = \E[Y(t)Y(u)] = \\ \E\left[\intinf h(\tau_1)X(t-\tau_1)\dtau_1
      \intinf h(\tau_2)X(u-\tau_2)\dtau_2\right] \\
  \end{empheq}

  Additional condition for interchange is finite mean-square value: $R_X(0) = \E[X^2(t)]<\infty$
  \begin{empheq}[box=\eqbox]{gather*}
      R_Y(\tau) = \intinf\intinf h(\tau_1)h(\tau_2)R_X(\tau-\tau_1+\tau_2)\dtau_1\dtau_2
  \end{empheq}

  (WS) stationary input process $X(t)$ to a stable LTI filter $\implies$ (WS) stationary output process $Y(t)$.

  \subsection{Power spectral density}
  \begin{empheq}[box=\eqbox]{equation*}
    S_{X}(f) = \mathscr{F}[R_{X}(\tau)](f) = \intinf R_{X}(\tau)e^{-j2\pi f\tau}\dtau 
        \end{empheq}
  \setlength{\leftmargini}{0.5cm} 
  \begin{itemize}
    \item $S_{X}(0) = \int_{-\infty}^{\infty} R_{X}(\tau)d\tau $
    \item $E[X^{2}(t)] = R_{X}(0) =  \int_{-\infty}^{\infty} S_{X}(f)df $
    \item $S_{X}(f) \ge 0$ $\forall f$
    \item $S_{X}(f)  = S_{X}(-f)$ $\forall f$, iff $X(t) \in \mathbb{R}$
  \end{itemize}
  \setlength{\leftmargini}{0pt} 

  \subsection{Gaussian process}
  Consider the r.v. $Y=\int_0^Tg(t)X(t)\dt$ where $g(t)$ is in an arbitraty function. If $Y$ is gaussian distributed, then the process $X(t)$  is a \textit{Gaussian process}
  \begin{itemize}
    \item A filtered Gaussian process remains a Gaussian process
    \item If $X(t)$ is a GP, the arbitrary set of r.v. $\vec{X} = [X(t_1),\dots,
    X(t_n)]^T$ is jointly gaussian distributed for any $n$
    \item The joint cdf is of these r.v. is completely determined by the \textbf{means} $\mu_X(t_i) = \E[X(t_i)]$ and \textbf{covariances} $C_X(t_k,t_i)=\E[(X(t_k)-\mu_X(t_k))(X(t_i)-\mu_X(t_i))]$
  \end{itemize}

  Multivariative Guass distribution:
  \begin{empheq}[box=\eqbox]{equation*}
    \begin{gathered}
      f(x) = \frac{\exp\left(-\frac{1}{2} (\vec{x}-\vec{m}_{x})^{T}\underline{\Sigma}^{-1} (\vec{x}-\vec{m}_{x}) \right)}{(2\pi)^{\frac{n}{2}}\det(\underline{\Sigma})^{\frac{1}{2}}}\\
      \underline{\Sigma} := 
      \begin{bmatrix} 
        \Cov(X_{1},X_{1})&...&\Cov(X_{1},X_{n})\\
        \vdotswithin{\ldots} & \vdotswithin{\ldots} & \vdotswithin{\ldots}\\
        \Cov(X_{n},X_{1})&...&\Cov(X_{n},X_{n})
      \end{bmatrix}
          \end{gathered}
  \end{empheq}

  \subsection{Noise}
  White noise is defined by its autocorrelation.
  \begin{empheq}[box=\eqbox]{align*}
      R_W(\tau) &= \frac{N_0}{2}\delta(t) & S_W(f) =  \frac{N_0}{2}
  \end{empheq}

  % ---------------------------------------------------------------------------
  % Week 2
  \section{Baseband Pulse Transmission}
  % ---------------------------------------------------------------------------
  Digital Baseband Pulse Transmission System: Based on the sample $y(t_i)$ 
  the receiver generates an estimate $\hat{a}_i$ of the amplitude $a_i$ of the
  transmitted pulse $g(t-iT_b)$.
  \cgraphic{1}{img/transmissionsystem.png}
  
  \subsection{Matched Filter}
  \cgraphic{0.9}{img/matchedfilter.png}
  \begin{empheq}{gather*}
      y(t) = g_0(t) + n(t) = h(t)*g(t) + h(t)*w(t)
  \end{empheq}

  Maximize \textit{pulse signal-to-noise ratio} $\eta$ at sampling time $t=T$:
  \begin{empheq}{gather*}
      \eta = \frac{\abs{g_0(T)}^2}{\E[n^2(t)]} = 
        \frac{\abs{\intinf H(f)G(f)e^{j2\pi fT}\df}^2}{\frac{N_0}{2}\intinf\abs{H(f)}^2\df}
  \end{empheq}

  Using Schwarz's inequality:
  \begin{empheq}[box=\eqbox]{gather*}
      \eta \leq \frac{2}{N_0}\intinf\abs{G(f)}^2\df
  \end{empheq}

  The quality sign (optimum) holds if $a(x)\propto b^*(x)$, i.e.
  \begin{empheq}[box=\eqbox]{gather*}
      H_{\text{opt}}(f) = k G^*(f)e^{-j2\pi fT} \Rightarrow h_{\text{opt}}(t) = k g(T-t)
  \end{empheq}

  The impulse response of the optimum filter, except for the scaling factor $k$, 
  is a time-reversed and delayed version of the input signal $g(t)$.

  The pulse SNR of a machted filter depends only on the ratio of the signal 
  energy $E$ to the PSD of the white noise at the input filter.
  \begin{empheq}{gather*}
      \eta_\text{max} = \frac{2}{N_0}\intinf \abs{G(f)}^2 \df = \frac{2E}{N_0} \\
      E = \intinf \abs{g(t)}^2\dt = \intinf \abs{G(f)}^2\df
  \end{empheq}

  \subsection{Error Rate}
  Discussed for a binary bipolar non-return-to-zero (NRZ) signal with amplitude $A$,
  bit duration $T_b$.
  \begin{empheq}{align*}
      x(t) &= \begin{cases}
              +A + w(t),& \text{Symbol 1 transmitted} \\
              -A + w(t),& \text{Symbol 0 transmitted}
              \end{cases} \\
      p_{\text{10}} &= \frac{1}{2}\erfc\left(\frac{A+\lambda}{\sqrt{N_0/T_b}}\right) =
       \Q\left(\sqrt{2}\frac{A+\lambda}{\sqrt{N_0/T_b}}\right) \\
      % {} &= \text{P}(y>\lambda | \text{symbol 0 was sent})
      {} &= \P(y>\lambda | \text{symbol 0 was sent})
  \end{empheq}

  The avg. prob. of symbol error $P_e$:
  \begin{empheq}[box=\eqbox]{gather*}
      P_e = \frac{p_0}{2} \erfc\left(\frac{A+\lambda}{\sqrt{N_0/T_b}}\right) + 
        \frac{p_1}{2} \erfc\left(\frac{A-\lambda}{\sqrt{N_0/T_b}}\right)
  \end{empheq}

  The error function:
  \begin{empheq}{align*}
      \P(n>a) \equiv \Q\left(\frac{a}{\sigma_n}\right) = \frac{1}{2}\erfc\left(\frac{1}{\sqrt{2}}\frac{a}{\sigma_n}\right)
  \end{empheq}

  Optimum decision threshold $\lambda$ that maximizes $P_e$:
  \begin{empheq}[box=\eqbox]{gather*}
      \lambda_\text{opt} = \frac{N_0}{4AT_b}\log\left(\frac{p_0}{p_1}\right)
  \end{empheq}

  \subsection{Intersymbol Interference}
  % \cgraphic{0.9}{img/isi.png}
  Arises when the channel is \textit{dispersive}, the magn. freq. resp. is not constant over the
  range of interest.
  \begin{empheq}{align*}
      s(t) &= \sum_k a_k \cdot g(t-kT_b) \\
      y(t) &= \mu\sum_k a_k\cdot p(t-kT_b) + n(t) \\
      t(t_i) &= \underbrace{\mu a_i}_\text{$i$-th bit} + \underbrace{\sum_{\substack{k=-\infty\\ k\neq i}}^{\infty} a_kp(i-k)T_b}_\text{ISI} + n(t_i)
  \end{empheq}

  \subsection{Nyquist's Criterion}
  In order to avoid ISI, we require $p(mT_b)=0$ for $m\neq0$ and obtain
  \begin{empheq}{gather*}
      \sum_{m=-\infty}^\infty p(mT_b)\delta(t-mT_b) = \delta(t) \laplace P_\delta(f)=1
  \end{empheq}

  An the nyquist criterion ($R_b = 1/T_b$ symbol rate):
  \begin{empheq}[box=\eqbox]{gather*}
      \sum_{n=-\infty}^\infty P(f-nR_b) = T_b
  \end{empheq}

  In words: The pulse function $P$ in freq. domain copied with spacing $R_b$ must be constant.

  \textbf{Ideal nyquist channel:} The simplest function $P(f)$ that satisfies this is the rectangular function (ideal LPF) with $W=R_b/2$, $R_b$ the nyquist rate.
  \begin{empheq}{align*}
      P(f) &= \frac{1}{2W}\rect\left(\frac{f}{2W}\right) = \begin{cases}
        \frac{1}{2W},&-W\leq f\leq W \\
        0,& \abs{f} > W
      \end{cases} \\
      p(t) &= \sinc(2Wt) \quad W = \frac{1}{2T_b} \quad E_b = \frac{A^2}{R_b}
  \end{empheq}

  \textbf{Raised Cosine Spectrum:} consists of flat portion and sinusodial rolloff.
  \cgraphic{0.8}{img/raisedcosine.png}
  \begin{empheq}{align*}
      P(f) &= \begin{cases}
        \frac{1}{2W} &  0\leq \abs{f} \leq f_1 \\
        \frac{1}{4W}\left(1-\sin\left[\frac{\pi(\abs{f}-W)}{2W-2f_1}\right]\right) 
          & \abs{f}\in[f_1, 2W-f_1]\\
        0 & \abs{f} > 2W-f_1
      \end{cases} \\
      p(t) &= \sinc(2Wt)\left(\frac{\cos(2\pi\alpha Wt)}{1-16\alpha^2W^2t^2}\right)\\
      \alpha &= 1-\frac{f_1}{W}  \in [0,1] \quad \text{Rollof factor}
  \end{empheq}

  Bandwidth is larger: $B_T = 2W-f_1 = W(1+\alpha$).

  \subsection{Correlative-Level Coding}
  Use basepulses which introduce controlled ISI. Same BW but higher $P_e$
  \cgraphic{1.0}{img/clc.png}
  \begin{empheq}{align*}
      H_I(f) &= \begin{cases}
        2T_b\cos(\pi fT_b)e^{-i\pi fT_b}, & \abs{f}<1/2T_b \\
        0, \text{else}
      \end{cases} \\
      h_I(t) &= \frac{T_b^2\sin(\pi t/T_n)}{\pi t(T_b-t)}
  \end{empheq}

  Decoding
  \cgraphic{0.6}{img/clc_dec.png}

  \textbf{Precoding} The decision feedback receiver is prone to propagating error. Using modulo-2 
  precoding, this can be omitted.
  \cgraphic{1}{img/precoding_oneline.png}
  \begin{empheq}[box=\eqbox]{gather*}
      d_k = b_k \oplus d_{k-1} \Rightarrow b_k = d_k \oplus d_{k-1} \\
      c_k = \begin{cases}
        0,&  b_k = 1\\
        \pm2,&  b_k = 0\\
      \end{cases}
  \end{empheq}

  \subsection{Baseband M-ary PAM Transmission}
  In a M-ary PAM system: M possible amplitude levels. One symbol encodes $\log_2M$ bits. 
  Thus the signal rate $T$ is related to the bit duration $T_b$ of a binary PAM as:
  \begin{empheq}{gather*}
      T = T_b\log_2M
  \end{empheq}
  \begin{itemize}
    \item For same avg. $P_e$, an M-ary PAM requires more Tx power
    \item If $M \gg 2$ the Tx energy per bit must be increased by $M^2/(3\log_2M)$ 
      for same $P_e$
  \end{itemize}


  % ---------------------------------------------------------------------------
  % Week 3
  \section{Signal Space Analysis}
  % ---------------------------------------------------------------------------
  Continuous AWGN (Additive white gaussian noise) channel.
  \begin{itemize}
    \item All symbols $m_i$ from source are eually likely $p_i=p(m_i)=\frac{1}{M}$
    \item Transmitter codes each $m_i$ into a signal $s_i(t)\in \{s_k(t) | 1\leq k \leq M\}$
    \item Cahnnel adds AWGN $x(t) = s_i(t) + w(t)$ for $0\leq t\leq T$
    \item The optimal receiver minimizes the avg. pob. of symbol error $P_e$
  \end{itemize}
  \begin{empheq}{gather*}
    P_e = \sum_{i=1}^M p_i \P(\hat{m}\neq m_i | m_i)
  \end{empheq}


  \subsection{Geometric Signal Representation}
  Let $\{\phi_i(t)\}_{i=1\dots N}$ be a set of othonormal basis fuctions of the signal set
  $\{s_i(t)\}_{i=1\dots M}$. All signals can be expressed as a finite sum.
  The coeff. $s_{ij}$ are given by the projection onto $\{\phi_i(t)\}_{i=1\dots N}$.

  The orthonormal functions deine a $N$-dimensional Eucledian space - the signal space.
  \begin{empheq}[box=\eqbox]{gather*}
      \int\limits_0^T \phi_i(t)\phi_j(t)\dt = \delta_{ij} = \begin{cases}
        1,&  i=j\\
        0,&  -\neq j\\
      \end{cases} \\
      s_i(t) = \sum_{j=1}^N s_{ij}\phi_j(t)  \quad
      s_{ij} = \int_0^T s_i(t)\phi_j(t)\dt\\
      0\leq t \leq T,\quad i=1\dots M,\quad j=1\dots N
  \end{empheq}

  \cgraphic{0.8}{img/sssynth.png}

  \begin{empheq}{gather*}
      \langle s_i(t), s_k(t) \rangle = \int_0^T s_i(t)s_k(d)\dt = \vect{s}_i^\top \cdot \vect{s}_k \\
      \norm{\vect{s}_i}^2 = \langle s_i(t), s_i(t) \rangle = \int_0^T s_i(t)^2 \\
      \norm{\vect{s}_i-\vect{s}_k}^2 = \sum_{j=1}^N(s_{ij}-s_{kj})^2 = \int_0^T (s_i(t)-s_k(t))^2 \dt \\
      \cos\theta_{jk} = \frac{\vect{s}_i^\top \cdot\vect{s}_k}{\norm{\vect{s}_i}\cdot\norm{\vect{s}_k}} \quad
      E_i = \sum_{j=1}^Ns_{ij}^2 = \norm{\vect{s}_i}^2
  \end{empheq}

  \textbf{Gram-Schmidt orthogonalization procedure}: Start with a complete system $s_1(t),\dots,s_M(t)$
  that generates the signal space. At each step generate a new basis function $\phi_i$.
  The basis has only $N\leq M$ functions.

  \begin{enumerate}
    \item Build basis function $\phi_1$ from $s_1$
    \begin{align*}  
      \phi_1(t) = \frac{s_1(t)}{\sqrt{\int_0^Ts_1^2(t)\dt}}
    \end{align*} 
    
    \item Search for a basis function from $s_2(t)$
    \begin{align*}  
      s_{21} &= \langle s_2(t), \phi_1(t) \rangle = \int_0^Ts_2(t)\phi_1(t)\dt \\
      g_2(t) &= s_2(t) - s_{21}\phi_1(t)
    \end{align*} 
    If $g_2=0$, $s_2$ is lin. dep. on $\phi_1$ and does not lead to a new basis function.
    Otherwise:
    \begin{align*}  
      \phi_2(t) = \frac{g_2(t)}{\sqrt{\int_0^Tg_2^2(t)\dt}}
    \end{align*} 
    
    \item Search for a basis function from $s_3(t)$
    \begin{align*}  
      s_{31} &= \langle s_3(t), \phi_1(t) \rangle \quad
      s_{32} = \langle s_3(t), \phi_2(t) \rangle\\
      g_3(t) &= s_3(t) - s_{31}\phi_1(t) - s_{32}\phi_2(t)
    \end{align*} 
    If $g_3=0$, $s_3$ is lin. dep. on $\phi_1$ and $\phi_2$ and does not lead to a new basis function.
    Otherwise:
    \begin{align*}  
      \phi_3(t) = \frac{g_3(t)}{\sqrt{\int_0^Tg_3^2(t)\dt}}
    \end{align*} 

    \item Search for a basis function from $s_M(t)$. Project $s_M$ on the already determined
    basis functions, decompose $S_M$ into its projection and a difference term $g_M$. 
    If $g_M\neq 0$:
    \begin{align*}  
      \phi_N(t) = \frac{g_M(t)}{\sqrt{\int_0^Tg_M^2(t)\dt}}
    \end{align*} 

  \end{enumerate}

  \subsection{Discrete System Model}
  The signal vector $\vect{s}$, noise vector $\vect{w}$ and the received signal $\vect{x}$.
  \begin{empheq}{gather*}
      \vect{s}_i = \begin{bmatrix} s_{i1} & \hdots & s_{iN} \end{bmatrix}^\top \quad
      \vect{w}   = \begin{bmatrix} w_{1} & \hdots & w_{N} \end{bmatrix}^\top \\
      \vect{x}   = \begin{bmatrix} x_{1} & \hdots & x_{N} \end{bmatrix}^\top = \vect{s}_i + \vect{w} \\
      \E[w_j] = 0 \quad \E[w_j\cdot w_k] = \delta_{jk} \quad \Var(w_j) = \frac{N_0}{2}
  \end{empheq}

  % \cgraphic{0.4}{img/awgnvectorchannel.png}
  \textbf{Theorem of Irrelevance} For signal detection with AWGN, only the projection of the noise
  onto the basis functions of the signal set $\{s_i(t)\}_{i=1}^M$ affect the sufficient statistics
  of the detection problem. The remainder of the noise is irrelevant.
  \begin{empheq}[box=\eqbox]{gather*}
      \mu_{X_j} = \E[X_j] = \E[s_{ij} + W_j] = s_{ij} + \E[W_j] = s_{ij} \\
      \sigma_{X_j}^2 = \Var(X_j) = \E[(X_j-s_{ij)^2}] = \E[W_j^2] = \frac{N_0}{2} \\
      W_j = \int_0^TW(t)\phi_j(t)\dt
  \end{empheq}

  The elements $X_j$ and $X_k$ of the received signal vector have the covariance
  \begin{empheq}{gather*}
      \Cov(x_j,x_k) = \E[(x_j-\mu_{x_j})(x_k-\mu_{x_k})] = 0, \quad j\neq k
  \end{empheq}
  Thus the $x_j$ are mutually uncorrelated. $\implies$ statistical independence.

  \textbf{Likelihood Function} As the $x_j$ are statistically indep. the conditional PDF of $\vect{x}$
  given $\vect{s}$ (i.e. symbol $m_i$ sent usign signal $s_i$) follows:
  \begin{empheq}[box=\eqbox]{gather*}
      L(\vect{s}_i) \coloneqq f_x(\vect{x}|\vect{s}_i) = f_W(\vect{w} = \vect{x}-\vect{s}_i) = 
      %= \prod_{j=1}^Nf_W(w_j=x_j-s_{ij})
      \\
       = \frac{1}{(\pi N_0)^{N/2}}\exp\left[-\frac{1}{N_0}\sum_{j=1}^N(x_j-s_{ij})^2\right] \\
       l(\vect{s}_i) = \log L(\vect{s}_i) = -\frac{1}{N_0}\sum_{j=1}^N(x_j-s_{ij})^2 + c
  \end{empheq}
  \begin{empheq}{gather*}
      c = -\frac{N}{2}\log(\pi N_0) \quad i\in \{1,\dots,M\}
  \end{empheq}

  $L$ likelihood function, $l$ log-likelihood function can be used because the pdf is always nonnegative
  and monot. incr.
  The constant $c$ is indep. of hyp. $\vect{s}_i$ and can be discarded for the decision.

  \subsection{Detection and Decoding}
  Detection problem: Given the observation $\vect{x}$, determine an estimate $\hat{m}$ of the transmitted
  symbol $m_i$, s.t. the probability of error is minimized.
  \cgraphic{0.5}{img/nosiecloud.png}

  \begin{empheq}[box=\eqbox]{gather*}
      P_e(m_i | \vect{x}) = \P(m_i \text{not sent} | \vect{x}) = 1-\P(m_i\text{sent} | \vect{x}) \\
  \end{empheq}
  
  The MAP (Maximum-A-Posteriori) decision rule is optimum in the minimum prob. of error sense. Set $\hat{m}=m_i$ if:
  \begin{empheq}[box=\eqbox]{gather*}
    \P(m_i\text{sent}|\vect{x}) \geq \P(m_k\text{sent}|\vect{x}) \quad \forall k\neq i
  \end{empheq}

  Rephrased using Baye's rule, set $\hat{m}=m_i$ if ($p_k$ a priori prob. of transmitting $m_k$, 
  $f_x(\vect{x}|m_k)$ cond. pdf of $\vect{x}$ given $m_k$):
  \begin{empheq}[box=\eqbox]{gather*}
    \hat{m} = \argmax_{m_k} \frac{p_k\cdot f_x(\vect{x}|m_k)}{f_x(\vect{x})} \quad \forall k\neq i
  \end{empheq}

  We can drop $f_x(\vect{x})$ as it is indep. of the symbol decision. For equiprobable source symbols,
  we obtain the ML decision rule: Set $\hat{m}=m_i$ if $l(m_k)$ max. for $k=i$.

  \textbf{Simplfied ML Rule}: $\vect{x}$ lies in region $Z_i$ if
  \begin{empheq}{gather*}
      \sum_{j=1}^Nx_js_{kj}-\frac{1}{2}E_k
  \end{empheq}
  is maximum for $k=i$.

  \textbf{Correlation receiver}
  \cgraphic{1.0}{img/crrelationrx.png}

  \subsection{Probability of Error}
  $\P(A_{ik})=P_2(\vect{s}_i,\vect{s}_k)$ is the pairwise error prob. that observation $\vect{x}$ is closer
  to $\vect{s}_k$ than to $\vect{s}_i$:
  \begin{empheq}{gather*}
    P_2(\vect{s}_i,\vect{s}_k) = \P(\norm{\vect{x}-\vect{s}_k}^2 < \norm{\vect{x}-\vect{s}_i}^2)
  \end{empheq}
  
  With the eucledian distance $d_{14}\coloneqq\norm{\vect{s}_1-\vect{s}_4}$:
  \begin{empheq}[box=\eqbox]{gather*}
    P_2(\vect{s}_1,\vect{s}_2) = \P(z<\frac{1}{2}d_{14}) = Q\left(\frac{d_{14}}{\sqrt{2M_0}}\right) \\
    = \frac{1}{2}\erfc\left(\frac{d_{14}}{2\sqrt{N_0}}\right)
  \end{empheq}

  \textit{The pairwise probability of error only depends on the Euclidean distance and is e.g. invariant 
  to rotation and translation of the signal constellation}

  From the union bound we have
  \begin{empheq}{gather*}
    P_e(m_i) \leq \sum_{\substack{k=1\\k\neq i}}^MP_2(\vect{s}_i,\vect{s}_k)
  \end{empheq}

  $P_e$ is the error prob. averaged over all symbols. An upper bound follows as
  \begin{empheq}{gather*}
    P_e = \sum_{i=1}^Mp_iP_e(m_i)\leq\frac{1}{2}\sum_{i=1}^M\sum_{\substack{k=1\\k\neq i}}^Mp_i\erfc\left(\frac{d_{ik}}{2\sqrt{N_0}}\right)
  \end{empheq}

  % ---------------------------------------------------------------------------
  % Week 4
  \section{Passband Data Transmission}
  % ---------------------------------------------------------------------------
  In bandpass data transmission, information modulates a carrier and occupies
  a restricted bandwidth in frequency. The carrier can be modulated by changing:
  \begin{itemize}
    \item Amplitude (ASK)
    \item Phase (PSK)
    \item Frequency (FSK)
  \end{itemize}

  \textbf{Cherent} modulation is when the receiver's local oscillator is 
  phase-synchronous to the transmitter's local oscillator.

  $M=s^n$ levels for signalling information ($M$-ary xSK). Using $M$ levels,
  symbol duration $T=nT_b$ is changed whilek eeping the same datarate. Bandwidth
  shrinks accordingly ny $1/nT_b$.

  Figures of merit: Symbol error probability at given SNR, power spectral density,
  bandwidth efficiency $\rho=R_b/B\quad [\text{bit/s/Hz}]$.

  \subsection{PSK: Coherent Phase Shift Keying}
  \textbf{BPSK: Binary PSK}
  % \cgraphic{0.8}{img/psk.png}

  \begin{empheq}{gather*}
    \phi_1(t) = \sqrt{\frac{2}{T_b}}\cos(2\pi f_ct) \\
    s_1(t) = \sqrt{E_b}\phi_1(t) \quad s_2(t) = - \sqrt{E_b}\phi_1(t) 
  \end{empheq}

  \cgraphic{0.8}{img/psktxrx.png}

  \begin{empheq}[box=\eqbox]{gather*}
    p_{10} = p_{01} = P_e = \frac{1}{2}\erfc\left(\sqrt{\frac{E_b}{N_0}}\right) \\
    S(f-f_c) \approx 2E_b\sinc^2(T_bf)
  \end{empheq}

  \textbf{QPSK: Quadriphase SK}, use more than just two phase levels.
  
  \begin{empheq}{gather*}
    \phi_1(t) = \sqrt{\frac{2}{T_b}}\cos(2\pi f_ct) \quad \phi_2(t) = \sqrt{\frac{2}{T_b}}\sin(2\pi f_ct) \\
    \vect{s}_{10} = \begin{bmatrix}  +c \\ +c \end{bmatrix} \smallskip
    \vect{s}_{00} = \begin{bmatrix}  -c \\ +c \end{bmatrix} \smallskip
    \vect{s}_{01} = \begin{bmatrix}  -c \\ -c \end{bmatrix} \smallskip
    \vect{s}_{11} = \begin{bmatrix}  +c \\ -c \end{bmatrix} \smallskip \\
    c = \sqrt{E/2}
  \end{empheq}

  \cgraphic{0.8}{img/qpsk.png}

  Every QPSK symbol carries 2 bits, hence the symbol energy is twice the energy per information bit: $E=2E_b$.
  A QPSK system achieves same BER as a BPSK at same $E_b/N_0$ but at \textit{twice the bit rate}.

  \begin{empheq}[box=\eqbox]{gather*}
    \text{BER} = \frac{1}{2}\erfc\left(\sqrt{\frac{E_b}{N_0}}\right) \\
    S_B(f) = 4E_b\sinc^2(2T_bf)
  \end{empheq}

  \subsection{QAM: Hybrind Amplitude/Phase Modulation}
  \textbf{QAM: M-ary quadrature amplitude modulation}, change phase and amplitude.

  $d_\text{min}$ is the distance between adjacent messages in the signal space.
  \begin{empheq}{gather*}
    \phi_1(t) = \sqrt{\frac{2}{T_b}}\cos(2\pi f_ct) \quad \phi_2(t) = \sqrt{\frac{2}{T_b}}\sin(2\pi f_ct) \\
    \vect{s}_{i} = \frac{d_{\text{min}}}{2}\begin{bmatrix} a_i \\ b_i \end{bmatrix} \smallskip a_i,b_i \smallskip \text{odd integers}, \smallskip i=1,\dots,M
  \end{empheq}

  Mapping an even number f bits per symbol (e.g. 4bits $\to$ 16 symbolds),
  results in a quadratic $L\times L$ square constellation with $L=\sqrt{M}$.
  Gray coding is often used for mapipng the bits to the QAM symbols.

  \cgraphic{0.7}{img/16qam.png}

  \begin{empheq}[box=\eqbox]{gather*}
    P_c = (1-P_e^\prime)^2 \to P_e = 1-P_c=1-(1-P_e^\prime)^2\approx2P_e^\prime\\
    P_e^\prime=\left(1-\frac{1}{\sqrt{M}}\right)\erfc\left(\sqrt{\frac{d_{\text{min}}^2}{4N_0}}\right) \\
    E_\text{av} = \frac{(M-1)d_{\text{min}}^2}{6} \\
    P_e\approx2P_e^\prime = 2 \left(1-\frac{1}{\sqrt{M}}\right)\erfc\left(\sqrt{\frac{3E_\text{av}}{2(M-1)N_0}}\right)
  \end{empheq}
  $E_\text{av}$ average symbol energy.

  \subsection{FSK: Coherent Frequency-Shoft Keying}

  \begin{empheq}{gather*}
    \phi_i(t) = \begin{cases}
      \sqrt{\frac{2}{T_b}}\cos(2\pi f_it), & 0\leq t \leq T_b \\
      0, & \text{else}.
    \end{cases} \\
    f_i = \frac{n_c+i}{T_b} \quad i=1,2, \quad n_c \in \N \\
    \vect{s}_1 = \sqrt{E_b} \begin{bmatrix} 1 \\ 0 \end{bmatrix} \quad
    \vect{s}_2 = \sqrt{E_b} \begin{bmatrix} 0 \\ 1 \end{bmatrix}
  \end{empheq}
  $f_i$ chosen by rule to avoid phase discontinuities. The two frequencies $f_1$
  and $f_2$ are $1/T_b$ Hz appart. The $\phi_i$ are orthogonal for $f_i=(n_c+i)/T_b$.

  \cgraphic{0.7}{img/bfsktxrx.png}

  Distance between message points in signal space is $1/\sqrt{2}$ smaller compared
  to binary PSK.

  \begin{empheq}[box=\eqbox]{gather*}
    d_\text{min} = \sqrt{2E_b} \\
     P_e = \frac{1}{2}\erfc\left(\sqrt{\frac{(d_\text{min}/2)^2}{N_0}}\right) = \frac{1}{2}\erfc\left(\sqrt{\frac{E_b}{2N_0}}\right) \\
     S_B(f) = \frac{E_b}{2T_b}\left[\delta\left(f-\frac{1}{2T_b}\right) + \delta\left(f+\frac{1}{2T_b}\right)\right] + \dots \\ 
      \frac{8E_b\cos^2(\pi T_b f)}{\pi^2(4T_b^2f^2-1)^2}
  \end{empheq}

  PSD contains two delta pulses and decays much faster than BPSK due to continuous phase operation.

  \cgraphic{0.7}{img/fskpsd.png}

  % Start of week 5
  \subsection{CPFSK Continuous Phase FSK}
  \begin{empheq}{gather*}
    s(t) = \sqrt{\frac{2E_b}{T_b}}\cos(2\pi f_ct + \theta(t)) \\
    \theta(t) = \theta(0) \pm \frac{\pi h}{T_b}t,\quad 0\leq t \leq T_b \\
    h = T_b(f_1-f_2), \quad f_c = \frac{1}{2}(f_1+f_2)
  \end{empheq}

  $h$ modulation index.
  
  \cgraphic{0.7}{img/cpfsk.png}

  \subsection{MSK Minimum Shift Keying}
  For integer valued $h$, the accumulated phase at end of symbol is independent
  of the previous and current symbol. $\leadsto$ no phase memory, each symbol can be
  decoded independently.
  \begin{empheq}{gather*}
    f_1 - f_2 = 0.5 / T_b
  \end{empheq}
  The minimum difference, for which $s_1(t), s_2(t)$ orthogonal.


  \begin{empheq}{align*}
    &\theta(0) = 0 & &\theta(T_b) = \pi/2 & & \text{symbol 1 transmitted} \\
    &\theta(0) = \pi & &\theta(T_b) = \pi/2 & & \text{symbol 0 transmitted} \\
    &\theta(0) = -\pi & &\theta(T_b) = -\pi/2 & & \text{symbol 1 transmitted} \\
    &\theta(0) = 0 & &\theta(T_b) = -\pi/2 & & \text{symbol 0 transmitted} \\
  \end{empheq}

  \textbf{Estimation of $\theta(0)$}: Expanding $s(t)$ into two terms we get:
  \cgraphic{0.7}{img/thetaesti.png}

  We can estimte $\theta(0)$ by observing
  \begin{empheq}{gather*}
    \sqrt{\frac{2E_b}{T_b}}\cos(\theta(t))\cos(2\pi f_c t)
  \end{empheq}

  \textbf{MSK Signal-Space representation}
  \begin{empheq}{gather*}
    \phi_1(t) = \sqrt{\frac{2}{T_b}} \cos\left(\frac{\pi}{2T_b}t\right)\cos(2\pi f_ct), \quad -T_b\leq t \leq T_b \\
    \phi_2(t) = \sqrt{\frac{2}{T_b}} \sin\left(\frac{\pi}{2T_b}t\right)\sin(2\pi f_ct), \quad 0\leq t \leq 2T_b
  \end{empheq}

  A coherent receiver has to integrate over two bit periods:
  \begin{empheq}{gather*}
    x_1 = \int_{-T_b}^{T_b} x(t)\phi_1(t)\dt = \sqrt{E_b}\cos(\theta(0)) + w_1 \\
    x_2 = \int_{0}^{2T_b} x(t)\phi_2(t)\dt = -\sqrt{E_b}\sin(\theta(T_b)) + w_2 \\
  \end{empheq}

  \textbf{Bit error rate}
  The four points in the signal-space diagram correspond to two symbol, hence the 
  BER is the same as with QPSK.
  \begin{empheq}{gather*}
    \text{BER} = \frac{1}{2}\erfc\left(\frac{d_\text{min}/2}{\sqrt{N_0}}\right) = \frac{1}{2}\erfc\left(\sqrt{\frac{E_b}{N_0}}\right)
  \end{empheq}
  
  \cgraphic{0.7}{img/mskpsd.png}

  \subsection{GMSK}
  To make sidelobes of MSK smaller, filter the NRZ signal with pulse shaping function.

  \begin{empheq}[box=\eqbox]{gather*}
    H(f) = \exp\left[{-\frac{\log2}{2}\left(\frac{f}{W}\right)^2}\right]
  \end{empheq}

  \cgraphic{0.7}{img/gmsk.png}

  The parameter $\alpha$ depends on the time bandwidth product $WT_b$. The quantity
  $10\log(\alpha/2)$ expresses the degradation in dB of GMSK compared to MSK.
  MSK: $WT_b=\infty,\alpha=0$
  \begin{empheq}{gather*}
    P_e = \frac{1}{2}\erfc\left(\sqrt{\frac{\alpha E_b}{2N_0}}\right)
  \end{empheq}

  \subsection{Equivalent baseband representation}
  \textbf{QAM}: Two branches: inphase (I) and quadrature (Q)

  \cgraphic{1.0}{img/qam.png}

  By using complex valued signals, the transmission system can be written as an LTI system.

  \cgraphic{1.0}{img/qamltidetail.png}

  Important names and notation:

  \begin{tabularx}{\textwidth}{l X}
    $\tilde{s}_\text{TX}(t)$ & compex envelope of $s_\text{TX}$ \\
    $s_\text{TX+}(t)$ & analytic signal (pre-envelope of $s_\text{TX}$) \\
    $s_\text{TX}(t)$ & physical passband signal \\
  \end{tabularx}

  \begin{empheq}{gather*}
    S_{TX+}(f) = \begin{cases}
        2S_\text{TX}(f) & f>0 \\ 0 & \text{else}
      \end{cases} \\
    H_{BP+}(f) = \begin{cases}
        H_\text{BP}(f) & f>0 \\ 0 & \text{else}
      \end{cases} \\
    H_\text{TX}(f) = 0\forall \abs{f}\geq f_0 \quad  H_\text{RX}(f) = 0\forall \abs{f}\geq B \\
    B = \max(0, f_0 - \abs{f_0-f_1})
  \end{empheq}

  \textbf{Complex Envelope} Inphase and quadrature components.
  \begin{empheq}{gather*}
    \tilde{s}_\text{TX}(t) = \tilde{s}_\text{TX,I}(t) + j\tilde{s}_\text{TX,Q}(t)
    \quad
    \tilde{S}_\text{TX}(f) = 0 \forall \abs{f} > f_0
  \end{empheq}

  \textbf{Analytic signal} Pre-envelope. Has one-sided spectrum, scaling factor to
  preserve power values in passband and qeuivalent baseband.
  \begin{empheq}{gather*}
    s_\text{TX+}(t) = \tilde{s}_\text{TX}(t)\sqrt{2}\exp(j\omega_0t)\exp(j\phi_0)\\
    S_\text{TX+}(f) = \sqrt{2}\exp(j\phi_0)\tilde{S}_\text{TX}(f-f_0) \\
    S_\text{TX+}(f) = 0 \forall f < 0 \\
    \tilde{S}_\text{TX}(f) = \frac{1}{\sqrt{2}}\exp(-j\phi_90)S_\text{TX+}(f+f_0)
  \end{empheq}


  \textbf{Physical passband signal} 
  \begin{empheq}{gather*}
    s_\text{TX}(t) = \Re\{s_\text{TX+}(t)\} \\
    S_\text{TX}(f) = \frac{1}{2}\left(S_\text{TX+}(f)+S_\text{TX+}^*(-f)\right) \\
    s_\text{TX}(t) = \sqrt{2}\tilde{s}_\text{TX,I}(t)\cos(\omega_0t+\phi_0)- \\
      \quad \sqrt{2}\tilde{s}_\text{TX,Q}(t)\sin(\omega_0t+\phi_0) \\
    s_\text{TX}(t) = \left\{ \sqrt{2}\sqrt{\tilde{s}_\text{TX,I}^2(t) + \tilde{s}_\text{TX,Q}^2(t)} \right\} \cos(\omega_0t+\phi_0+\phi(t)) \\
    \phi(t) = \atan2(\tilde{s}_\text{TX,Q}(t), \tilde{s}_\text{TX,I}(t))
  \end{empheq}
  
  \textbf{Summary}
  \begin{empheq}[box=\eqbox]{gather*}
    x(t) = \Re\{x_\text{+}(t)\} \quad x_\text{+}(t) = \tilde{x}(t)\sqrt{2}\e^{j2\pi ft}
  \end{empheq}
  \begin{tabularx}{\textwidth}{l X}
    $x(t)$ & physical passband signal \\
    $x_\text{+}(t)$ & analytic signal (pre-envelope of $x(t)$) \\
    $\tilde{x}(t)$ & compex envelope of $x(t)$ \\
  \end{tabularx}

  \subsection{Noncoherent Detection}
  Carrier phase $\theta$ at the receiver becomes a random variable.

  \subsection{ML detection with unknown phase shift}
  \begin{empheq}{gather*}
    L(\vect{s}_i) \triangleq f_{X}(\vect{x} | \vect{s}_i) = \\
      \frac{1}{(\pi N_0)^{N/2}}\exp\left[-\frac{1}{N_0}\sum_{j=0}^N(x_j-s_{ij})^2\right]
  \end{empheq}
  
  The ML receiver selects the hypothesis, which maximizes the likelihood function
  \begin{empheq}{gather*}
    \hat{i} = \argmax_i \left(L(\vect{s}_i)\right)
  \end{empheq}

  Expanding the sum in the exponent, the likelihood function can be calculated from the output
  of a correlator bank.
  \begin{empheq}[box=\eqbox]{gather*}
    L(\vect{s}_i) = c\exp\left[ \frac{2}{N_0}\int x(t)s_i(t)\dt - \frac{1}{N_0}E_{s_i} \right]
  \end{empheq}
  With $E_{s_i}=\sum_j s_{ij}^2=\int s_i^2\dt$

  For a known phase offset, the modified receiver correlates with a rotate version of each hypothesis.
  \cgraphic{0.8}{img/uncoherent_ml.png}

  \textbf{Two-branch correlator}
  \begin{empheq}{gather*}
    s_i(t,\theta) = s_i(t,\theta=0)\cos\theta - s_i(t,\theta=-\pi/2)\sin\theta
  \end{empheq}

  \cgraphic{0.8}{img/two_branch_correlator.png}

  \textbf{Equi-Energy Signals with unknown phase offset}
  Shifting the integrator to each branch and obtain equi-energy signals with known phase offset:
  \begin{empheq}{align*}
    L(\vect{s}_i | \theta) &= \exp\left(\frac{1}{N_0}\left(a_c\cos\theta - a_s\sin\theta\right)\right) \\
     &= \exp\left(\frac{1}{N_0}\sqrt{a_c^2+a_s^2}\cos(\theta+\phi)\right) \\
     \phi &= \angle(a_c+ja_s)
  \end{empheq}

  With unknoen phase offset, we have to average the likelihood function across all phase offsets $\theta$. 
  \begin{empheq}[box=\eqbox]{align*}
    \overline{L(\vect{s}_i)} &= \frac{1}{2\pi}\int_{\pi}^{\pi}\exp\left(\frac{1}{N_0}\sqrt{a_c^2+a_s^2}\cos(\theta+\phi)\right)\dtheta \\
    &= I_0\left(\frac{1}{N_0}\sqrt{a_c^2 + a_s^2}\right)
  \end{empheq}

  $I_0$ is the modified Bessel function of order zero.
  \cgraphic{0.8}{img/besselcorrelator.png}

  As $I_0$ is monotonously increasing, a simplified decision rule follows as
  \begin{empheq}[box=\eqbox]{align*}
    \hat{i} = \argmax_i d_i^2
  \end{empheq}

  Note that we need a two-branch correlator for each hypothesis $s_i(t)$.

  Instead of the two-branch correlator we can use two matched filter - sampler pairs to 
  calculate the decision variable.

  We can determine the decision variable woth one matched filter an a Hilbert tranformer.
  The matched filter - envelope detector pair is called a noncoherent matched filter.
  \begin{empheq}{align*}
    s_i(t,\theta=-\pi/2)\laplace S_i&(f,\theta=-\pi/2) \\&= -j\sgn(f)S_i(f,\theta=0)
  \end{empheq}

  \cgraphic{0.999}{img/noncoherenthilbert.png}

  \subsection{Noncoherent FSK}
  Signal $x(t)$ at the receiver with unknown carrier phase offset $\theta$:
  \begin{empheq}{gather*}
    x(t)=sqrt{\frac{2E_b}{T_b}}\cos(2\pi f_it+\theta)+w(t),\quad i=1,2,0\leq t\leq T_b
  \end{empheq}

  The signals $s_1$ and $s_2$ each require such a branch. A comparator subsequently compares the
  two outputs $I_i$ to devide between the hypothesis $s_1$ and $s_2$.

  \cgraphic{1}{img/noncoherent_fsk.png}

  We have
  \begin{empheq}{gather*}
    P_e = \frac{1}{2}\exp\left(-\frac{E_b}{2N_0}\right)
  \end{empheq}
  This corresponds to a degradation of at least 3dB compared to coherent MSK. Less
  degradation compared to BFSK.

  Another implementation is with matched bandpass filters to $f_1$ and $f_2$ followed
  by envelope detectors, samplers and a comparison device.
  \cgraphic{0.8}{img/noncoherent_matchedfilt_fsk.png}

  \subsection{DPSK Differential PSK}
  Differential precoding at transmitter: Symbol 0 $\leadsto$ $\pi$ phasejump, Symbol 1 $\leadsto$ no phase-jump.
  Assumption: $\theta$ does not change significantly between two adjecent sampling instances.

  \begin{empheq}{align*}
    s_1(t,\theta) &= \begin{cases}
      \sqrt{\frac{2E_b}{T_b}}\cos(2\pi f_ct+\theta),&0\leq t\leq T_b \\
      \sqrt{\frac{2E_b}{T_b}}\cos(2\pi f_ct+\theta),&T_b\leq t\leq 2T_b \\
    \end{cases} \\
    s_2(t,\theta) &= \begin{cases}
      \sqrt{\frac{2E_b}{T_b}}\cos(2\pi f_ct+\theta),&0\leq t\leq T_b \\
      \sqrt{\frac{2E_b}{T_b}}\cos(2\pi f_ct+\pi+\theta),&T_b\leq t\leq 2T_b \\
    \end{cases} \\
  \end{empheq}

  Noncoherent detector for DPSK:
  \cgraphic{0.8}{img/noncohdpsk.png}

  Quadrature implementation of simplified detector:
  \cgraphic{0.8}{img/quadraturenoncoherent.png}

  DPSK is a special case of noncoherent, orthogonal modulation with $T=2T_b$ and $E=2E_b$. Th 
  bit error rate is given by:
  \begin{empheq}{gather*}
    P_e = \frac{1}{2}\exp\left(-\frac{E_b}{N_0}\right)
  \end{empheq}


  \subsection{Performance comparison}
  \begin{tabular}{l | l}
    Modulation & $P_e$ \\ \hline \hline
    Coherent BPSK & \multirow{3}{*}{$\frac{1}{2}\erfc\left(\sqrt{\frac{E_b}{N_0}}\right)$} \\
    Coherent QPSK & \\
    Coherent MSK& \\ \hline
    Coherent binary FSK & $\frac{1}{2}\erfc\left(\sqrt{\frac{E_b}{2N_0}}\right)$ \\ \hline
    DPSK & $\frac{1}{2}\exp\left(-\frac{E_b}{N_0}\right)$ \\ \hline
    Noncoherent binary FSK & $\frac{1}{2}\exp\left(-\frac{E_b}{2N_0}\right)$ \\ \hline
  \end{tabular}
  \cgraphic{1}{img/bercompare.png}

  % ---------------------------------------------------------------------------
  \section{Multi User Radio Communications}
  % Week 7
  % ---------------------------------------------------------------------------
  \subsection{Multiple Access techniques}
  \cgraphic{0.8}{img/muxtech.png}
  Accomodation of several users in the same wireless environment.

  \begin{itemize}
    \item FDMA Frequency domain multiple access
    \item TDMA Time domain multiple access
    \item CDMA Code division multiple access
    \item SDMA Spatial division multiple access
  \end{itemize}

  \subsection{Radio Communication over line-of-sight (LOS)}
  Free-space (line of sight) communication on up- and down-link.
  AWGN model appropriate. Used MA techniques:
  \begin{itemize}
    \item \textbf{FDMA} 
    Non linearity of transponder causes interference between frequency subband.
    Transponder is operated in lin. regime below max output power. Reduced power efficiency.
    \item \textbf{TDMA}
    Can operate at close to full power efficiency. Commonly used.
    \item \textbf{SDMA}
    Multiple antennas allow beam forming to different lcoations.
  \end{itemize}

  \textbf{Link budget}
  Link (power) budget: Budgeting of all gains and losses. Accounting of resources available to transmitter
  and receiver, sources of loss of power, sources of noise. Allows performance estimation of LOS links.

  \textbf{Link Margin}
  Curve relates $P_e$ to $E_b/N_0$. Max accepted $P_e$ leads to Op1 and minimal required $(E_b/N_0)_\text{req}$.
  Actually received $(E_b/N_0)_\text{rec}$ define Op2.
  \cgraphic{0.8}{img/linkmargin.png}

  Link margin: Difference. Provides Protection agains unexpected changes. Large margin, high reliability but low efficiency.
  \begin{empheq}[box=\eqbox]{gather*}
    M = 10\log\left(\frac{E_b}{N_0}\right)_\text{rec} - 10\log\left(\frac{E_b}{N_0}\right)_\text{req} \quad [M] = \text{dB}
  \end{empheq}
  \cgraphic{1.0}{img/linkbudget.png}

  \subsection{Antenna characterization}
  Receiver is located in the farfield of the transmitter. $D$ largest dimenstion of antenna.
  \begin{empheq}{gather*}
    d_f \gg \frac{2D^2}{\lambda}
  \end{empheq}

  Idealized reference antenna radiates uniformly in all directions. Power density as a function of distance
  scales according to \text{free-space propagation}. $\Phi$ Radiation intensity in watts per unit solid angle.
  \begin{empheq}{gather*}
    \rho(d) = \frac{P_t}{4\pi d^2} \quad [\rho]=\frac{\text{W}}{\text{m}^2} \quad \Phi = d^2\rho(d)
  \end{empheq}

  Total radiated power $P$ and average power per unit solid angle $P_{av}$:
  \begin{empheq}{gather*}
    P=\int\Phi(\theta,\phi)\dOmega \quad P_{av} = \frac{1}{4\pi}\int\Phi(\theta,\phi)\dOmega=\frac{P}{4\pi}\\
    [P]=\text{W} \quad [P_{av}]=\frac{\text{W}}{\text{steradian}}
  \end{empheq}

  \textbf{Directivity gain} $g(\theta,\phi)$ Ratio of radiation intensity in a specific direction to the
  avg. radiated power.

  \textbf{Directivity} $D$, maximal directivity gain over all directions

  \textbf{Power gain} $G$ with $\eta_\text{radiation}\in[0,1]$ the radiation efficiency factor.
  
  \textbf{EIRP} Effective isotropically radiated power referenced to an isotropic source.

  \textbf{Beamwidth} Angle between the two directions in which the radiation intensity is one-half the maximum.
  Higher power gain leads to narrower beamwidth.

  \textbf{Effective Apperture} $A$ Ratio of power available at the antenna terminals to the power per unit area of the appropriately polarized incident electromagnetic wave.

  \textbf{Apperture efficiency} $\eta_{ap}$ whith $A_{ph}$ physical area.

  \begin{empheq}{gather*}
    g(\theta,\phi) = \frac{\Phi(\theta,\phi)}{P_{av}} = \frac{\Phi(\theta,\phi)}{P/(4\pi)} \quad D=\max_{\theta,\phi} g(\theta, \phi) \\
    G=\eta_\text{radiation}D \quad \EIRP=P_tG_t \\
    \quad A=(\lambda^2/4\pi)G \quad \eta_{ap}=A/A_{ph}
  \end{empheq}

  \textbf{Frii's Free-Space Equation}
  Power captured by receiver at distance $d$ in LoS:
  \begin{empheq}[box=\eqbox]{gather*}
    P_r = \left(\frac{\EIRP}{4\pi d^2}\right)A_r = \frac{P_tG_tA_r}{4\pi d^2} = P_tG_tG_r\left(\frac{\lambda}{4\pi d}\right)^2
  \end{empheq}

  \textbf{Path loss}
  $\text{PL}$ is the difference between transmit signal power and receive signal power.
  \begin{empheq}[box=\eqbox]{gather*}
    \text{PL} = 10\log\frac{P_t}{P_r}=-10\log(G_tG_r)+10\log\left(\frac{4\pi d}{\lambda}\right)^2
  \end{empheq}

  \subsection{Noise figure}
  Spot noise figure $F(f)$ ratio of total available output noise power per unit bandwidth to portion thereof due to 
  source alone. $G$ power gain.
  \begin{empheq}[box=\eqbox]{gather*}
    F(f) = \frac{S_{NO}(f)}{G(f)S_{NS}(f)} = \frac{P_S S_{NO}}{P_O S_{NS}} = \frac{\SNR_{\text{Source}}(f)}{\SNR_\text{Output}(f)}
  \end{empheq}

  If two-port is noise free:
  \begin{empheq}{gather*}
    S_{NO}(f) = G(f)S_{NS}(f)
  \end{empheq}

  For physical systems
  \begin{empheq}{gather*}
    S_{NO}(f) > G(f)S_{NS}(f)
  \end{empheq}

  \textbf{Equivalent Noise Temperature} $T_e$. For low noise devices $T_e$ is a better measure bcs $F$ is close to unity.
  Two-port device matched to source impedance is considered. $N_1$ available input noise power:
  \begin{empheq}{gather*}
    N_1 = \left(\frac{\sqrt{4kTR_sB}}{2R_s}\right)^2R_s=kTB.
  \end{empheq}

  Total output noise power $N_2$ and noise figure:
  \begin{empheq}{gather*}
    N_2 = GN_1+N_d = Gk(T+T_e)B\quad \\ F=\frac{N_2}{N_2-N_d}=\frac{T+T_e}{T}
  \end{empheq}

  Equivalent noise temperature $T_e$
  \begin{empheq}[box=\eqbox]{gather*}
    T_e = T(F-1)
  \end{empheq}

  \textbf{Cascade of Two-Port Networks}
  \begin{empheq}[box=\eqbox]{gather*}
    F = F_1 + \frac{F_2-1}{G_1} + \frac{F_3-1}{G_1G_2} + \frac{F_4-1}{G_1G_2G_3}+\cdots\\
    T_e=T_{e1}+\frac{T_{e2}}{G_1}+\frac{T_{e3}}{G_1G_2}+\frac{T_{e4}}{G_1G_2G_3}+\cdots
  \end{empheq}

  \subsection{Radio Communication over multipath channnels}
  In mobile radio systems the transmitters and receivers are mobile. 
  This leads to a stocahstic channel. Multipath phenomenenon leads to \textbf{Fading}.

  \textbf{Narrowband fading} 
  Complex envelopes ot $\tilde{s}$ Tx, $\tilde{s_o}$ Rx signal and $\tilde{h}$ timevarying impulse response of channel.
  The Rayleigh fading model for NLOS conditions is modeled as zero-mean complex Gaussian random process.
  Characterized by autocorrelation function $R_{\tilde{h}}$ and Doppler spectrum $S_{\tilde{H}}$.
  \begin{empheq}[box=\eqbox]{gather*}
    \tilde{S}_o(t) = \intinf\tilde{s}(t-\tau)\tilde{h}(\tau;t)\dtau \\
    \tilde{h}(\tau;t)=\tilde{h}(t)\delta(\tau)\leadsto \tilde{s}_o(t)=\tilde{h}(t)\tilde{s}(t) \\
    R_{\tilde{h}} (\Delta t) = \E[\tilde{h}^*(t)\tilde{h}(t+\Delta t)] \laplace S_{\tilde{H}}(\nu)
  \end{empheq}

  \textbf{Coherent BPSK over slow rayleigh fading channel} 
  Input-output relation is $\tilde{s}_o(t)=\alpha\exp(-j\phi)\tilde{s}(t)+\tilde{w}(t)$
  with $\alpha$ and $\phi$ Rayleigh and uniformly distributed r.v.
  \begin{empheq}{gather*}
    P_{e|\tilde{h}}(\gamma)=\frac{1}{2}\erfc\sqrt{\gamma}\quad\gamma=\frac{\alpha^2E_b}{N_0}
  \end{empheq}

  Averaging over all channel realizations:
  \begin{empheq}{gather*}
    P_e(\gamma_0)=\int\limits_0^\infty P_{e|\tilde{h}}(\gamma)f(\gamma)\dgamma = \frac{1}{2}\left(1-\sqrt{\frac{\gamma_0}{1-\gamma_0}}\right)\\
    \gamma_0=\E[\gamma]=\frac{E_b}{N_0}\E[\alpha^2]
  \end{empheq}

  \cgraphic{1.0}{img/fadinggraph.png}

  At high SNR deep fades dominate performance $P_e\propto 1/SNR$.

  \textbf{Diversity} Availability of independently faded copies of the transmit signal at the receiver.
  Diversity order $L$ number of available indep. faded versions of the same signal.
  At high SNR, $L$-th order diversity allows $P_e\propto 1/SNR^L$

  \subsection{Summary}
  \begin{empheq}[box=\eqbox]{gather*}
    N_0=kTF \quad P_n = N_0B \quad \SNR_{\text{dB}} = P_{r,\text{dB}} - P_{n,\text{dB}} \\
    P_r = P_tG_tG_r\left(\frac{\lambda}{4\pi d}\right)^2 \\
    P_r = RE_{b,min}M \quad A= G_r\frac{\lambda^2}{4\pi}=\eta_{ap}A_{ph}
  \end{empheq}
  $N_0$ Noise power spectral density [W/Hz] \\
  $F$ Noise Figure \\
  $B$ Bandwidth [Hz] \\
  $P_n$ Noise power [W] \\
  $P_r/P_t$ Receiver/Transmitter power [W] \\
  $G_r/G_t$ Receiver/Transmitter antenna gain \\
  $d$ Distance between receiver/transmitter \\
  $R$ Datarate \\
  $E_{b,min}$ Minimum energy per bit \\
  $M$ Link margin \\
  $A$ Effective apperture \\
  $\eta_{ap}$ Apperture efficiency \\
  $A_{ph}$ Physical area of antenna \\
  
  % ---------------------------------------------------------------------------
  % Information Theory
  \section{Information Theory}
  % ---------------------------------------------------------------------------

  \subsection{Uncertainty, Information and Entropy}
  A source emits a message $S$. $S$ is a r.v. taking values in a finite alphabet $S=\{s_0,\dots,s_{K-1}\}$.
  \begin{empheq}{gather*}
    \P(S=s_k) = p_k \quad \sum_{k=0}^{K-1} p_k = 1
  \end{empheq}

  The definition of information $I$ is:
  \begin{empheq}[box=\eqbox]{gather*}
    I(s_k) \triangleq -\log p_k
  \end{empheq}
  \begin{empheq}{align*}
    &I(s_k) = 0 &&\text{for} \quad p_k=1 \\
    &I(s_k) \geq 0 &&\text{for} \quad 0\leq p_k \leq 1 \\
    &I(s_k) > I(s_i) &&\text{for} \quad p_k < p_i \\
    &I(s_ks_i)= I(s_k) + I(s_i) &&\text{if stat. indep.} \quad p_{ki}=p_kp_i
  \end{empheq}

  When $I$ is specified in bits, logarithms are to base 2.

  The definition of entropy $H$ is:
  \begin{empheq}[box=\eqbox]{gather*}
    H(\mathcal{S}) \triangleq E[I(\mathcal{S})] = \sum_{k=0}^{K-1}p_kI(s_k) = -\sum_{k=0}^{K-1}p_k\log p_k
  \end{empheq}
  \begin{empheq}{align*}
    &0 \leq H(\mathcal{S}) \leq \log_2K \\
    &H(\mathcal{S})=0 &&\text{iff} \quad p_k=1 \quad\text{for one $k$} \\
    &H(\mathcal{S})=\log_2K &&\text{iff} \quad p_k=\frac{1}{K}\forall k \\
  \end{empheq}

  \textbf{Extended source} Divide a seq. of $n\cdot M$ successive source symbols into $M$ blocks.
  Consider each block of $n$ symbols as a single "super symbol" taking on values in $\mathcal{S}^n$. The entropy 
  of the extended source is:
  \begin{empheq}[box=\eqbox]{gather*}
    H(\mathcal{S}^n) = n\cdot H(\mathcal{S})
  \end{empheq}

  \subsection{Source Coding Theorem}
  \textit{How is the information of a source efficiently represented?}
  Requirements: Code words must be binary, unique decodability.
  \cgraphic{0.8}{img/sourcecoding.png}

  The average code word length of a  code whose $k$th code-word is of length $L_k$:
  \begin{empheq}{gather*}
    \bar{L}=\sum_{k=0}^{K-1}p_kL_k
  \end{empheq}

  The average code workd length is lower bounded by the entropy of the source.
  \begin{empheq}[box=\eqbox]{gather*}
    \bar{L}\geq H(\mathcal{S}) \triangleq L_{min}
  \end{empheq}

  Coding efficiency is a meausre of code quality:
  \begin{empheq}{gather*}
    \eta=\frac{L_{min}}{\bar{L}}\leq 1
  \end{empheq}

  \subsection{Data Compation}
  Goal: Elmininate redundancy. Seek for codes that approach Shannon's lower bound on the avg code-word length.

  \textbf{Prefix Codes}: No code-word is a prefix of another code-word. Leads to implicit
  recognition of end of code word. For each discrete, memoryless source there exists a prefix code s.t.:
  \begin{empheq}{gather*}
    H(\mathcal{S}) \leq \bar{L} < H(\mathcal{S})+1
  \end{empheq}
  \begin{empheq}{gather*}
    H(\mathcal{S}^n) \leq \bar{L}_n < H(\mathcal{S}^n)+1 \\
    \Leftrightarrow nH(\mathcal{S})\leq \bar{L}_n < nH(\mathcal{S})+1 \\
     \Leftrightarrow H(\mathcal{S}) \leq \frac{\bar{L}_n}{n}<H(\mathcal{S})+\frac{1}{n}
  \end{empheq}
  $\frac{\bar{L}_n}{n}$ effective nmber of bit per source symbol.

  \textbf{Huffman Coding} yields a prefix code that minimizes the avg. code-word length when the
  source is memoryless.
  \begin{enumerate}
    \item Assign a 0 and 1 to the symbols of lowest probability
    \item Replace the two symbols by a new pseudo-symbol whose prob. is the sum of the two probs.
    \item Repeat 1. and 2. until only a single pseudo-symbol left
  \end{enumerate}
  The code sequence for each symbol is found by backtracking from last symbol and tracing the 0s and 1s.
  \cgraphic{0.8}{img/huffman.png}

  Drawbacks:
  \begin{itemize}
    \item Need to know probabilities a priori
    \item Redundancy due to memory in source can only be removed by using large extension codes, increasing
    complexity
  \end{itemize}

  \textbf{Lempel-Ziv Coding} Adaptive algorithm of low complexity that captures the source statistic and memory
  in the source intrinsically.
  \cgraphic{1}{img/lempelziv.png}

  \begin{itemize}
    \item Constructed by parsingt he source data stream into segments other than 0 and 1 that
    are shortest subsequences not encountered previously
    \item Segment 0 and 1 are asigned indices 1 and 2
    \item $N$ stored subsequences are indezed from $3$ to $N+2$
    \item A new sequence can always be composed from an old seubsequence (root subsequence) and a 0
    and a 1 (innovation symbol)
  \end{itemize}


  \subsection{Discrete Memoryless Channel}
  \cgraphic{0.8}{img/dmchannel.png}

  $X,Y$ are statistically dependant r.v.
  Discrete if the input and output alphabet ($\mathcal{X}$, $\mathcal{Y}$) are of finite size
  Memoryless if the current output depends on the current input only.
  It is fully described by
  \begin{itemize}
    \item input alphabet $\mathcal{X}=\{x_0,\dots,x_{J-1}\}$
    \item output alphabet $\mathcal{Y}=\{y_0,\dots,y_{K-1}\}$
    \item transition probabilities $p(y_k|x_j) = \P(Y=y_k | X = x_j)$
  \end{itemize}

  \textbf{Transition Matrix}
  \begin{empheq}{gather*}
    \Pr=
      \begin{bmatrix} 
        p(y_0|x_0) & \cdots & p(y_{K-1} | x_0) \\
        \vdots & \ddots & \vdots \\
        p(y_0 | x_{J-1}) & \cdots & p(y_{K-1} | x_{J-1})
      \end{bmatrix} \\
      p(y_k) = \P(Y=y_k) = \sum_{j=0}^{J-1}p(y_k|x_j)p(x_j)
  \end{empheq}

  \textbf{Binary, Symmetric Channel}
  \cgraphic{0.6}{img/bsc.png}
  $J=K=2$, transition probability $p$. Error probability is $p$.


  % ---------------------------------------------------------------------------
  % Inf theory 2
  % ---------------------------------------------------------------------------
  \subsection{Mutual Information}
  \textbf{Conditional entropy} of $X$ given $Y$ is a measure for the uncertainty about $X$ if $Y$ is known:
  \begin{empheq}{gather*}
    H(X|Y) = -\E_{X,Y}[\log_2 p(X|Y)]=\\-\sum_{k=0}^{K-1}\sum_{j=0}^{J-1}p(x_j,y_k)\log_2 p(x_j|y_k)
  \end{empheq}

  \textbf{Mutual information} is the reduction of the uncertainty about $X$ achieved by observing $Y$.
  \begin{empheq}[box=\eqbox]{gather*}
    I(X;Y) \triangleq H(X)-H(X|Y)
  \end{empheq}
  \begin{empheq}{gather*}
    I(X;Y)=I(Y;X) \\
    H(X)-H(X|Y)=H(Y)-H(Y|X) \\
    I(X;Y) \geq 0 \\
    H(X)\geq H(X|Y)
  \end{empheq}

  \textbf{Joint entropy} of $X$ and $Y$ is defined as:
  \begin{empheq}{gather*}
    H(X,Y) = -\E_{X,Y}[\log_2 p(X,Y)]=\\-\sum_{k=0}^{K-1}\sum_{j=0}^{J-1}p(x_j,y_k)\log_2 p(x_j,y_k) \\
    I(X;Y) + H(X,Y) = H(X) + H(Y)
  \end{empheq}
  \cgraphic{0.6}{img/mutualinfo.png}

  \subsection{Channel Capacity}
  A channel is a statistical model with input $X$ and output $Y$. Mutual information depends also on
  $p(x)$.
  \begin{empheq}{gather*}
    I(X;Y) = H(Y)-H(Y|X) = \\\sum_{k=0}^{K-1}\sum_{j=0}^{J-1}p(x_j,y_k)\log\left(\frac{ p(y_k|x_j)}{p(y_k)}\right)=\\
    \sum_{k=0}^{K-1}\sum_{j=0}^{J-1}p(y_k|x_j)p(x_j)\log\left(\frac{ p(y_k|x_j)}{\sum_{j=0}^{J-1}p(y_k|x_j)p(x_j)}\right)
  \end{empheq}

  \textbf{Channel capacity} is the maximum mutual information over all possible input distributions:
  \begin{empheq}[box=\eqbox]{gather*}
    C \triangleq \max_{\{p(x_j)\}} I(X;Y)
  \end{empheq}
  \cgraphic{0.9}{img/capacity-binarychannel.png}

  \subsection{Channel Coding Theorem}
  \textbf{Shannon's Channel Coding Theorem}
  \begin{itemize}
    \item Consider discrete, memoryless source emitting values in $\mathcal{S}$
    \item One symbol emitted every $T_s$ seconds - information rate $H(\mathcal{S})/T_s$
    \item One coded symbol transmitter every $T_c$ seconds
  \end{itemize}
  \begin{tcolorbox}[width=\columnwidth,colback=orange1,arc=0pt]
   \underline{Theorem} If $H(\mathcal{S})/T_s < C/T_c$ there exists a channel code yielding an 
   arbitratily small block (message) error probability as the channel code-word length
   goes to infinity. For $H(\mathcal{S})/T_s \geq C/T_c$ such a code does not exist.
  \end{tcolorbox}

  \subsection{Differential Entropy}
  Idea: Source and channels with cintinuous alphabets.

  Differential entropy:
  \begin{empheq}[box=\eqbox]{gather*}
    h(X) = -\intinf f_X(x)\log f_X(x)\dx
  \end{empheq}

  The entropy $H(X)$ goes to $\infty$ in the limit of $\delta x\to 0$. But the mutual information
  is well defined:
  \begin{empheq}[box=\eqbox]{gather*}
    I(X;Y) = h(X) - h(X|Y)
  \end{empheq}

  Let $X\sim \mathcal{N}(\mu,\sigma^2)$ be a Gaussian random variable with variance $\sigma^2$. Then
  the diff. entropy is uniquely determined by its variance.
  \begin{empheq}{gather*}
    h(X) = \frac{1}{2}\log(2\pi e \sigma^2)
  \end{empheq}

  \subsection{Information Capacity Theorem}
  \textbf{Information capacity} is the maximum of the mutual information between input $X_i$ 
  and output $Y_i$ overa ll distributions of $X_i$ fulfilling the contraint:
  \begin{empheq}[box=\eqbox]{gather*}
    C_D = \max_{\{f_X(x)\}}\left\{I(X_i;Y_i):\E[X_i^2]=E_s\right\}
  \end{empheq}
  Where $E_s$ is interpreted as the average energy per symbol.
  \begin{empheq}{gather*}
    I(X_i;Y_i) = h(Yi)-h(Y_i|X_i) = h(X_i+N_i)-h(X_i+N_i | X_i) \\
    h(X_i + N_i) = \frac{1}{2}\log(2\pi e(E_s+\sigma^2)) \\
    h(N_i) = \frac{1}{2}\log(2\pi e \sigma^2) \\
    C_D = \frac{1}{2}\log_2\left(1+\frac{E_s}{\sigma^2}\right)
  \end{empheq}

  \subsection{Implications of the Inf. Capacity Thm.}
  \textbf{ToDo: } Not yet covered in lecture

  \subsection{Colored Noise Channel}
  \textbf{ToDo: } Not yet covered in lecture

  % ---------------------------------------------------------------------------
  % Basics
  \vfill\null
  \pagebreak
  \section{Math}
  % ---------------------------------------------------------------------------
  \subsection{General}
  \begin{empheq}{align*}
      &\cos(a)\cos(b) + \sin(a)\sin(b) = \cos(a-b) \\
      &\cos(a+b)\cos(a-b) = \frac{1}{2}\left[\cos(2a) + \cos(2b)\right] \\
      &\cos(a)\cos(b) = \frac{1}{2}( \cos(a-b) + \cos(a+b) ) \\
      &\sin(a)\sin(b) = \frac{1}{2}( \cos(a-b) - \cos(a+b) ) \\
      &\cos(a)\sin(b) = \frac{1}{2}( \sin(a+b) - \sin(a-b) ) \\
      &\sinc(x) = \frac{\sin(\pi x)}{\pi x} \\
      &\sin(x) = \frac{e^{ix}-e^{-ix}}{2i} \quad \cos(x) = \frac{e^{ix}+e^{-ix}}{2}
  \end{empheq}

  \subsection{Fourier Transform}
  Source: Haykin, Communication systems, 4th ed.
  \begin{empheq}{align*}
    &\rect\left(\frac{t}{T}\right) &&\laplace&& T\sinc(fT) \\
    &\sinc(2Wt) &&\laplace&& \frac{1}{2W}\rect\left(\frac{f}{2W}\right) \\
    &\exp(-at)u(t),a>0 &&\laplace&& \frac{1}{a+j2\pi f}\\
    &\exp(-a\abs{t}),a>0 &&\laplace&& \frac{2a}{a^2+(2\pi f)^2}\\
    &\exp(-\pi t^2) &&\laplace&& \exp( -\pi f^2)\\
    &\begin{cases}1-\frac{\abs{t}}{T},&\abs{t}<T\\0,&\abs{t}\geq T\end{cases} &&\laplace&& T\sinc^2(fT)\\
    &\delta(t) &&\laplace&& 1 \\
    &1 &&\laplace&& \delta(f) \\
    &\delta(t-t_0) &&\laplace&& \exp(-j2\pi ft_0)\\
    &\exp(j2\pi f_c t) &&\laplace&& \delta(f-f_c)\\
    &\cos(2\pi f_ct) &&\laplace&& \frac{1}{2}[\delta(f-f_c) + \delta(f+f_c)] \\
    &\sin(2\pi f_ct) &&\laplace&& \frac{1}{2i}[\delta(f-f_c) + \delta(f+f_c)] \\
    &\sgn(t) &&\laplace&& \frac{1}{j\pi f}\\
    &\frac{1}{\pi t} &&\laplace&& -j\sgn(f)\\
    &u(t) &&\laplace&& \frac{1}{2}\delta(f) + \frac{1}{j2\pi f} \\
    &\sum_{i=-\infty}^\infty \delta(t-iT_0) &&\laplace&& \frac{1}{T_0}\sum_{n=-\infty}^\infty \delta\left(f-\frac{n}{T_0}\right)
  \end{empheq}
  $u(t)$ unit step function \\
  $\delta(t)$ delta function \\
  $\rect(t)$ rectangular function of unit amplitude and unit duration centered on the origin \\
  $\sgn(t)$ signum function \\
  $\sinc(t)$ sinc function \\

  Relations
  \begin{empheq}{align*}
    &\alpha f(t) + \beta g(t) &&\laplace&& \alpha F(f) + \beta G(f) \\
    &f^*(t) &&\laplace&& F^*(-f) \\
    &f(at) &&\laplace&& \frac{1}{\abs{a}}F\left(\frac{f}{a}\right) \\
    &f(t-a) &&\laplace&& \e^{-j2\pi fa} F(f) \\
    &e^{j2\pi f_0 f} &&\laplace&& F(f-f_0) \\
    &f^{(n)} &&\laplace&& (j2\pi f)^n F(f) \\
    &t^n f(t) &&\laplace&& j^n F^{(n)}(f) \\
    &\int_{-\infty}^t x(\tau) \dtau &&\laplace&& \frac{1}{j2\pi f} F(f) + \pi F(0)\delta(f) \\
    &\frac{1}{t} x(t) + \pi x(0)\delta(t) &&\laplace&& \int_{-\infty}^{f} X(s) \ds \\
    &(f*g)(t) &&\laplace&& F(f)\cdot G(f) \\
    &f(t)\cdot g(t) &&\laplace&& \frac{1}{2\pi} F(f) * G(f) \\
  \end{empheq}
  $f^{(n)}$ $n^\text{th}$ derivation \\
  $f^*$ complex conjugate

  \subsection{Probability}
  \begin{empheq}{align*}
    &\P(X > x) = \frac{1}{2}\erfc\left(\frac{x}{\sqrt{2}\sigma_X}\right) \\
    &\erf(x) = \frac{1}{\sqrt{\pi}}\int_{-x}^x \e^{-t^2}\dt = \frac{2}{\sqrt{\pi}}\int_{0}^x \e^{-t^2}\dt \\
    &\erfc(x) = 1 - \erf{x} = \frac{2}{\sqrt{\pi}}\int_{x}^\infty \e^{-t^2}\dt \\
  \end{empheq}



\end{multicols*}

\setcounter{secnumdepth}{2}
\end{document}
